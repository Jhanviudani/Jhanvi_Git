{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth, fpmax\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/rajshah/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ast \n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter out tokens based on English dictionary words\n",
    "\n",
    "def cleanTokens(total_tokens):\n",
    "    tokens = ast.literal_eval(total_tokens)\n",
    "    return [token for token in tokens if token in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>females</th>\n",
       "      <th>males</th>\n",
       "      <th>nbs</th>\n",
       "      <th>unknowns</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>total_others</th>\n",
       "      <th>talking_about_men</th>\n",
       "      <th>passesBechdel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30654</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, find, data, ship, main, unusual, momenta...</td>\n",
       "      <td>['computer', 'data']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5758</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[name, picked, ever, particularly, quickly, re...</td>\n",
       "      <td>['halliday', 'her', 'she']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42734</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[block, river, might, find, hole, two, data, m...</td>\n",
       "      <td>['anderson', 'maya']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40199</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[kibble, near, nowhere, feed, big, cat, escort...</td>\n",
       "      <td>['he', 'her', 'his']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15505</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[give, gun]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       chars  females  males  nbs  unknowns  \\\n",
       "30654      3        0      3    0         0   \n",
       "5758       1        0      1    0         0   \n",
       "42734      4        2      2    0         0   \n",
       "40199      2        1      1    0         0   \n",
       "15505      1        1      0    0         0   \n",
       "\n",
       "                                            total_tokens  \\\n",
       "30654  [sir, find, data, ship, main, unusual, momenta...   \n",
       "5758   [name, picked, ever, particularly, quickly, re...   \n",
       "42734  [block, river, might, find, hole, two, data, m...   \n",
       "40199  [kibble, near, nowhere, feed, big, cat, escort...   \n",
       "15505                                        [give, gun]   \n",
       "\n",
       "                     total_others  talking_about_men  passesBechdel  \n",
       "30654        ['computer', 'data']               True          False  \n",
       "5758   ['halliday', 'her', 'she']               True          False  \n",
       "42734        ['anderson', 'maya']               True          False  \n",
       "40199        ['he', 'her', 'his']               True          False  \n",
       "15505                          []              False          False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bechdelScenes = pd.read_csv('Dataset/allBechdelScenes.csv')\n",
    "mediaScenes = pd.read_csv('Dataset/allMediaDiversityScenes.csv')\n",
    "df = pd.concat([bechdelScenes, mediaScenes])\n",
    "\n",
    "# Define 'passesBechdel' column based on Bechdel test criteria\n",
    "\n",
    "df['passesBechdel'] = (df['chars'] >= 2) & (df['males'] <= 1) & (df['talking_about_men'] == False)\n",
    "# Apply token cleaning function to 'total_tokens' column\n",
    "\n",
    "df['total_tokens'] = df.apply(lambda x: cleanTokens(x.total_tokens), axis=1)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_bechdel_scenes = df[df.passesBechdel]\n",
    "fail_bechdel_scenes = df[df.passesBechdel == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_tokens = pass_bechdel_scenes['total_tokens'].tolist()\n",
    "fail_tokens = fail_bechdel_scenes['total_tokens'].tolist()\n",
    "# Perform Association Rule Mining on scenes that pass Bechdel test\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary1 = te.fit(pass_tokens).transform(pass_tokens)\n",
    "pass_transactions = pd.DataFrame(te_ary1, columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(pass_transactions, min_support=0.01, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1433</td>\n",
       "      <td>0.102595</td>\n",
       "      <td>(like, know)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>982</td>\n",
       "      <td>0.087376</td>\n",
       "      <td>(know, get)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1508</td>\n",
       "      <td>0.081940</td>\n",
       "      <td>(know, think)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1525</td>\n",
       "      <td>0.079766</td>\n",
       "      <td>(know, want)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1466</td>\n",
       "      <td>0.070254</td>\n",
       "      <td>(know, one)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>988</td>\n",
       "      <td>0.069303</td>\n",
       "      <td>(like, get)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1680</td>\n",
       "      <td>0.068759</td>\n",
       "      <td>(like, think)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1619</td>\n",
       "      <td>0.064954</td>\n",
       "      <td>(like, look)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1480</td>\n",
       "      <td>0.064683</td>\n",
       "      <td>(right, know)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1694</td>\n",
       "      <td>0.064411</td>\n",
       "      <td>(like, want)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   support       itemsets\n",
       "0   1433  0.102595   (like, know)\n",
       "1    982  0.087376    (know, get)\n",
       "2   1508  0.081940  (know, think)\n",
       "3   1525  0.079766   (know, want)\n",
       "4   1466  0.070254    (know, one)\n",
       "5    988  0.069303    (like, get)\n",
       "6   1680  0.068759  (like, think)\n",
       "7   1619  0.064954   (like, look)\n",
       "8   1480  0.064683  (right, know)\n",
       "9   1694  0.064411   (like, want)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_itemsets = frequent_itemsets[frequent_itemsets['itemsets'].str.len() > 1].reset_index(drop=True)\n",
    "frequent_itemsets = frequent_itemsets.sort_values(by='support', ascending=False).reset_index(drop=True)\n",
    "frequent_itemsets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before we go further, let's remove very common words from the dataset. We'll be using the advice given here: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>females</th>\n",
       "      <th>males</th>\n",
       "      <th>nbs</th>\n",
       "      <th>unknowns</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>total_others</th>\n",
       "      <th>talking_about_men</th>\n",
       "      <th>passesBechdel</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6212</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[guy, hard, deck, watch, interrupt, pull, alt,...</td>\n",
       "      <td>['rooster']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>guy hard deck watch interrupt pull alt coming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5113</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[since, hour, come, perfect, today, much, went...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>since hour come perfect today much went work g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24203</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[get, die, run, see, bitch, away]</td>\n",
       "      <td>['raizo', 'mika']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>get die run see bitch away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39410</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[two, find, last, companion, king, three, book...</td>\n",
       "      <td>['she', 'his', 'her', 'ofelia', 'he']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>two find last companion king three book echo s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17372</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[two, last, hope, would, whilst, math, crack, ...</td>\n",
       "      <td>['fischer', 'ariadne']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>two last hope would whilst math crack three ef...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       chars  females  males  nbs  unknowns  \\\n",
       "6212       1        0      1    0         0   \n",
       "5113       2        0      2    0         0   \n",
       "24203      3        1      2    0         0   \n",
       "39410      2        1      1    0         0   \n",
       "17372      5        0      4    1         0   \n",
       "\n",
       "                                            total_tokens  \\\n",
       "6212   [guy, hard, deck, watch, interrupt, pull, alt,...   \n",
       "5113   [since, hour, come, perfect, today, much, went...   \n",
       "24203                  [get, die, run, see, bitch, away]   \n",
       "39410  [two, find, last, companion, king, three, book...   \n",
       "17372  [two, last, hope, would, whilst, math, crack, ...   \n",
       "\n",
       "                                total_others  talking_about_men  \\\n",
       "6212                             ['rooster']               True   \n",
       "5113                                      []              False   \n",
       "24203                      ['raizo', 'mika']               True   \n",
       "39410  ['she', 'his', 'her', 'ofelia', 'he']               True   \n",
       "17372                 ['fischer', 'ariadne']               True   \n",
       "\n",
       "       passesBechdel                                               text  \n",
       "6212           False  guy hard deck watch interrupt pull alt coming ...  \n",
       "5113           False  since hour come perfect today much went work g...  \n",
       "24203          False                         get die run see bitch away  \n",
       "39410          False  two find last companion king three book echo s...  \n",
       "17372          False  two last hope would whilst math crack three ef...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token cleaning for more meaningful analysis\n",
    "\n",
    "df['text'] = df.apply(lambda x: (' ').join(x.total_tokens), axis = 1)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vals = df.text.values\n",
    "countVect = CountVectorizer()\n",
    "countVect = countVect.fit(text_vals)\n",
    "X_cv = countVect.transform(text_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdCV = pd.DataFrame(X_cv.toarray(), columns=countVect.get_feature_names_out())\n",
    "pdCV[pdCV > 0] = 1\n",
    "pdCV = pdCV.loc[:,(pdCV.sum(axis=0) > 15)]\n",
    "pdCV = pdCV.loc[:,(pdCV.sum(axis=0) < pdCV.shape[0]/5)]\n",
    "good_tokens = set(pdCV.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajshah/.pyenv/versions/3.9.18/lib/python3.9/site-packages/pandas/util/_decorators.py:311: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "pdCV = pd.read_csv('pdCV.csv', index_col=False)\n",
    "good_tokens = set(pdCV.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdCV.sample(5).to_csv('pdCV.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token cleaning function for scenes failing Bechdel test\n",
    "\n",
    "def cleanerTokens(tokens):\n",
    "    return [token for token in tokens if token in good_tokens]\n",
    "\n",
    "df['scrubbed_tokens'] = df.apply(lambda x: cleanerTokens(x.total_tokens), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_bechdel_scenes = df[df.passesBechdel]\n",
    "fail_bechdel_scenes = df[df.passesBechdel == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7359, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pass_bechdel_scenes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_bechdel_scenes = fail_bechdel_scenes.sample(pass_bechdel_scenes.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_tokens = pass_bechdel_scenes['scrubbed_tokens'].tolist()\n",
    "fail_tokens = fail_bechdel_scenes['scrubbed_tokens'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association Rule Mining on scenes failing Bechdel test\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary1 = te.fit(pass_tokens).transform(pass_tokens)\n",
    "pass_transactions = pd.DataFrame(te_ary1, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets = apriori(pass_transactions, min_support=0.01, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052317</td>\n",
       "      <td>(want, think)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.043756</td>\n",
       "      <td>(right, think)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.043484</td>\n",
       "      <td>(want, right)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.042533</td>\n",
       "      <td>(want, going)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042261</td>\n",
       "      <td>(going, think)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.041854</td>\n",
       "      <td>(well, think)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.041582</td>\n",
       "      <td>(want, got)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.039815</td>\n",
       "      <td>(want, see)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.039272</td>\n",
       "      <td>(think, good)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.039272</td>\n",
       "      <td>(right, got)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    support        itemsets\n",
       "0  0.052317   (want, think)\n",
       "1  0.043756  (right, think)\n",
       "2  0.043484   (want, right)\n",
       "3  0.042533   (want, going)\n",
       "4  0.042261  (going, think)\n",
       "5  0.041854   (well, think)\n",
       "6  0.041582     (want, got)\n",
       "7  0.039815     (want, see)\n",
       "8  0.039272   (think, good)\n",
       "9  0.039272    (right, got)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_itemsets = frequent_itemsets[frequent_itemsets['itemsets'].str.len() > 1].reset_index(drop=True)\n",
    "frequent_itemsets = frequent_itemsets.sort_values(by='support', ascending=False).reset_index(drop=True)\n",
    "frequent_itemsets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better - we can see more interesting results. Let's look at scenes that failed the Bechdel test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary2 = te.fit_transform(fail_tokens)\n",
    "fail_transactions = pd.DataFrame(te_ary2, columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.077320</td>\n",
       "      <td>(want, right)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.077184</td>\n",
       "      <td>(want, think)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.075282</td>\n",
       "      <td>(right, think)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.074874</td>\n",
       "      <td>(right, got)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.073108</td>\n",
       "      <td>(want, got)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.050414</td>\n",
       "      <td>(could, would)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.050143</td>\n",
       "      <td>(got, man)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.050007</td>\n",
       "      <td>(take, thing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.050007</td>\n",
       "      <td>(make, going)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.050007</td>\n",
       "      <td>(back, would)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      support        itemsets\n",
       "0    0.077320   (want, right)\n",
       "1    0.077184   (want, think)\n",
       "2    0.075282  (right, think)\n",
       "3    0.074874    (right, got)\n",
       "4    0.073108     (want, got)\n",
       "..        ...             ...\n",
       "131  0.050414  (could, would)\n",
       "132  0.050143      (got, man)\n",
       "133  0.050007   (take, thing)\n",
       "134  0.050007   (make, going)\n",
       "135  0.050007   (back, would)\n",
       "\n",
       "[136 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_itemsets_fail = apriori(fail_transactions, min_support=0.05, use_colnames=True)\n",
    "frequent_itemsets_fail = frequent_itemsets_fail[frequent_itemsets_fail['itemsets'].str.len() > 1].reset_index(drop=True)\n",
    "frequent_itemsets_fail = frequent_itemsets_fail.sort_values(by='support', ascending=False).reset_index(drop=True)\n",
    "frequent_itemsets_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(want)</td>\n",
       "      <td>(think)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.052317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(think)</td>\n",
       "      <td>(want)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.052317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(right)</td>\n",
       "      <td>(think)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(think)</td>\n",
       "      <td>(right)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(want)</td>\n",
       "      <td>(right)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(right)</td>\n",
       "      <td>(want)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(want)</td>\n",
       "      <td>(going)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(going)</td>\n",
       "      <td>(want)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(going)</td>\n",
       "      <td>(think)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(think)</td>\n",
       "      <td>(going)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(well)</td>\n",
       "      <td>(think)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(think)</td>\n",
       "      <td>(well)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(want)</td>\n",
       "      <td>(got)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(got)</td>\n",
       "      <td>(want)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(want)</td>\n",
       "      <td>(see)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.039815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(see)</td>\n",
       "      <td>(want)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.039815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(right)</td>\n",
       "      <td>(got)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.039272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(got)</td>\n",
       "      <td>(right)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.039272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(good)</td>\n",
       "      <td>(think)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.039272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(think)</td>\n",
       "      <td>(good)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.039272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   antecedents consequents  antecedent support  consequent support   support  \\\n",
       "0       (want)     (think)                 NaN                 NaN  0.052317   \n",
       "1      (think)      (want)                 NaN                 NaN  0.052317   \n",
       "2      (right)     (think)                 NaN                 NaN  0.043756   \n",
       "3      (think)     (right)                 NaN                 NaN  0.043756   \n",
       "4       (want)     (right)                 NaN                 NaN  0.043484   \n",
       "5      (right)      (want)                 NaN                 NaN  0.043484   \n",
       "6       (want)     (going)                 NaN                 NaN  0.042533   \n",
       "7      (going)      (want)                 NaN                 NaN  0.042533   \n",
       "8      (going)     (think)                 NaN                 NaN  0.042261   \n",
       "9      (think)     (going)                 NaN                 NaN  0.042261   \n",
       "10      (well)     (think)                 NaN                 NaN  0.041854   \n",
       "11     (think)      (well)                 NaN                 NaN  0.041854   \n",
       "12      (want)       (got)                 NaN                 NaN  0.041582   \n",
       "13       (got)      (want)                 NaN                 NaN  0.041582   \n",
       "14      (want)       (see)                 NaN                 NaN  0.039815   \n",
       "15       (see)      (want)                 NaN                 NaN  0.039815   \n",
       "18     (right)       (got)                 NaN                 NaN  0.039272   \n",
       "19       (got)     (right)                 NaN                 NaN  0.039272   \n",
       "17      (good)     (think)                 NaN                 NaN  0.039272   \n",
       "16     (think)      (good)                 NaN                 NaN  0.039272   \n",
       "\n",
       "    confidence  lift  leverage  conviction  zhangs_metric  \n",
       "0          NaN   NaN       NaN         NaN            NaN  \n",
       "1          NaN   NaN       NaN         NaN            NaN  \n",
       "2          NaN   NaN       NaN         NaN            NaN  \n",
       "3          NaN   NaN       NaN         NaN            NaN  \n",
       "4          NaN   NaN       NaN         NaN            NaN  \n",
       "5          NaN   NaN       NaN         NaN            NaN  \n",
       "6          NaN   NaN       NaN         NaN            NaN  \n",
       "7          NaN   NaN       NaN         NaN            NaN  \n",
       "8          NaN   NaN       NaN         NaN            NaN  \n",
       "9          NaN   NaN       NaN         NaN            NaN  \n",
       "10         NaN   NaN       NaN         NaN            NaN  \n",
       "11         NaN   NaN       NaN         NaN            NaN  \n",
       "12         NaN   NaN       NaN         NaN            NaN  \n",
       "13         NaN   NaN       NaN         NaN            NaN  \n",
       "14         NaN   NaN       NaN         NaN            NaN  \n",
       "15         NaN   NaN       NaN         NaN            NaN  \n",
       "18         NaN   NaN       NaN         NaN            NaN  \n",
       "19         NaN   NaN       NaN         NaN            NaN  \n",
       "17         NaN   NaN       NaN         NaN            NaN  \n",
       "16         NaN   NaN       NaN         NaN            NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "association_pass = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.001, support_only=True)\n",
    "association_pass.sort_values(by='support', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(want)</td>\n",
       "      <td>(right)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.077320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(right)</td>\n",
       "      <td>(want)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.077320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(want)</td>\n",
       "      <td>(think)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.077184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(think)</td>\n",
       "      <td>(want)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.077184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(right)</td>\n",
       "      <td>(think)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(think)</td>\n",
       "      <td>(right)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(right)</td>\n",
       "      <td>(got)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.074874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(got)</td>\n",
       "      <td>(right)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.074874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(want)</td>\n",
       "      <td>(got)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.073108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(got)</td>\n",
       "      <td>(want)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.073108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(got)</td>\n",
       "      <td>(think)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.072972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(think)</td>\n",
       "      <td>(got)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.072972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(see)</td>\n",
       "      <td>(back)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(back)</td>\n",
       "      <td>(see)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(want)</td>\n",
       "      <td>(see)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.072428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(see)</td>\n",
       "      <td>(want)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.072428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(got)</td>\n",
       "      <td>(back)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.071885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(back)</td>\n",
       "      <td>(got)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.071885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(see)</td>\n",
       "      <td>(right)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.071341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(right)</td>\n",
       "      <td>(see)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.071341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   antecedents consequents  antecedent support  consequent support   support  \\\n",
       "0       (want)     (right)                 NaN                 NaN  0.077320   \n",
       "1      (right)      (want)                 NaN                 NaN  0.077320   \n",
       "2       (want)     (think)                 NaN                 NaN  0.077184   \n",
       "3      (think)      (want)                 NaN                 NaN  0.077184   \n",
       "4      (right)     (think)                 NaN                 NaN  0.075282   \n",
       "5      (think)     (right)                 NaN                 NaN  0.075282   \n",
       "6      (right)       (got)                 NaN                 NaN  0.074874   \n",
       "7        (got)     (right)                 NaN                 NaN  0.074874   \n",
       "8       (want)       (got)                 NaN                 NaN  0.073108   \n",
       "9        (got)      (want)                 NaN                 NaN  0.073108   \n",
       "10       (got)     (think)                 NaN                 NaN  0.072972   \n",
       "11     (think)       (got)                 NaN                 NaN  0.072972   \n",
       "12       (see)      (back)                 NaN                 NaN  0.072700   \n",
       "13      (back)       (see)                 NaN                 NaN  0.072700   \n",
       "14      (want)       (see)                 NaN                 NaN  0.072428   \n",
       "15       (see)      (want)                 NaN                 NaN  0.072428   \n",
       "17       (got)      (back)                 NaN                 NaN  0.071885   \n",
       "16      (back)       (got)                 NaN                 NaN  0.071885   \n",
       "18       (see)     (right)                 NaN                 NaN  0.071341   \n",
       "19     (right)       (see)                 NaN                 NaN  0.071341   \n",
       "\n",
       "    confidence  lift  leverage  conviction  zhangs_metric  \n",
       "0          NaN   NaN       NaN         NaN            NaN  \n",
       "1          NaN   NaN       NaN         NaN            NaN  \n",
       "2          NaN   NaN       NaN         NaN            NaN  \n",
       "3          NaN   NaN       NaN         NaN            NaN  \n",
       "4          NaN   NaN       NaN         NaN            NaN  \n",
       "5          NaN   NaN       NaN         NaN            NaN  \n",
       "6          NaN   NaN       NaN         NaN            NaN  \n",
       "7          NaN   NaN       NaN         NaN            NaN  \n",
       "8          NaN   NaN       NaN         NaN            NaN  \n",
       "9          NaN   NaN       NaN         NaN            NaN  \n",
       "10         NaN   NaN       NaN         NaN            NaN  \n",
       "11         NaN   NaN       NaN         NaN            NaN  \n",
       "12         NaN   NaN       NaN         NaN            NaN  \n",
       "13         NaN   NaN       NaN         NaN            NaN  \n",
       "14         NaN   NaN       NaN         NaN            NaN  \n",
       "15         NaN   NaN       NaN         NaN            NaN  \n",
       "17         NaN   NaN       NaN         NaN            NaN  \n",
       "16         NaN   NaN       NaN         NaN            NaN  \n",
       "18         NaN   NaN       NaN         NaN            NaN  \n",
       "19         NaN   NaN       NaN         NaN            NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "association_fail = association_rules(frequent_itemsets_fail, metric='confidence', min_threshold=0.001, support_only=True)\n",
    "association_fail.sort_values(by='support', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Market Basket Analysis doesn't show us much - we are only able to see very common words still. Next step would be to do it by movies instead of scenes to see if that gives us better data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step: Analyzing by movies instead of scenes\n",
    "\n",
    "df = [] # Initializing empty list to store scene data\n",
    "for f in os.listdir('Dataset/allScenes'):\n",
    "    if not f.endswith('.csv'):\n",
    "        continue\n",
    "    filename = f.split('_scenes.csv')[0]\n",
    "    scenes = pd.read_csv('Dataset/allScenes/' + f, index_col=False).total_tokens.values\n",
    "    full_tokens = []\n",
    "    full_scenes = []\n",
    "\n",
    "    for scene in scenes:\n",
    "        scene_x = ast.literal_eval(scene)\n",
    "        full_scenes.append([token for token in scene_x if token in words])\n",
    "        full_tokens += scene_x\n",
    "    full_tokens = [token for token in full_tokens if token in good_tokens]\n",
    "    df.append({\n",
    "        'movie_filename': filename,\n",
    "        'tokens': full_tokens,\n",
    "        'text': (' ').join(full_tokens),\n",
    "        'scenes': full_scenes\n",
    "    })\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_filename</th>\n",
       "      <th>tokens</th>\n",
       "      <th>text</th>\n",
       "      <th>scenes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Easy-A</td>\n",
       "      <td>[go, two, town, every, video, confess, make, a...</td>\n",
       "      <td>go two town every video confess make account o...</td>\n",
       "      <td>[[go, two, town, neighboring, every, video, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Killers-Of-The-Flower-Moon-Read-The-Screenplay</td>\n",
       "      <td>[sacred, teaching, white, bury, gave, grandfat...</td>\n",
       "      <td>sacred teaching white bury gave grandfather ta...</td>\n",
       "      <td>[[sacred, teaching, white, bury, gave, pah, gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cast-Away</td>\n",
       "      <td>[pretty, gas, way, cut, mountain, filter, engi...</td>\n",
       "      <td>pretty gas way cut mountain filter engine gues...</td>\n",
       "      <td>[[pretty], [gas, way, get, cut, mountain, filt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ghost-Ship</td>\n",
       "      <td>[work, cabin, mind, friendship, find, main, ta...</td>\n",
       "      <td>work cabin mind friendship find main talk grad...</td>\n",
       "      <td>[[work, cabin, mind, friendship, find, know, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Downsizing</td>\n",
       "      <td>[afraid, happen, make, right, born, thing, old...</td>\n",
       "      <td>afraid happen make right born thing old give f...</td>\n",
       "      <td>[[afraid, know, happen, make, right, born, thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>Bourne-Ultimatum-The</td>\n",
       "      <td>[radio, give, argument, gun, would, last, ago,...</td>\n",
       "      <td>radio give argument gun would last ago pam thr...</td>\n",
       "      <td>[[radio, give, argument, gun], [would, last, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>Happy-Go-Lucky</td>\n",
       "      <td>[bit, dance, make, test, holding, celebrate, c...</td>\n",
       "      <td>bit dance make test holding celebrate child cr...</td>\n",
       "      <td>[[bit, dance, make, test, framing, holding, ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>Blind-Side-The</td>\n",
       "      <td>[investigate, trouble, bit, find, granger, fil...</td>\n",
       "      <td>investigate trouble bit find granger file wind...</td>\n",
       "      <td>[[investigate], [trouble, bit, find, know, gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>Croods-The</td>\n",
       "      <td>[find, last, hope, would, every, three, fun, n...</td>\n",
       "      <td>find last hope would every three fun neighbor ...</td>\n",
       "      <td>[[find, last, hope, would, every, three, fun, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Trainspotting</td>\n",
       "      <td>[television, opener, family, compact, machine,...</td>\n",
       "      <td>television opener family compact machine elect...</td>\n",
       "      <td>[[television, opener, family, compact, machine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>796 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     movie_filename  \\\n",
       "0                                            Easy-A   \n",
       "1    Killers-Of-The-Flower-Moon-Read-The-Screenplay   \n",
       "2                                         Cast-Away   \n",
       "3                                        Ghost-Ship   \n",
       "4                                        Downsizing   \n",
       "..                                              ...   \n",
       "791                            Bourne-Ultimatum-The   \n",
       "792                                  Happy-Go-Lucky   \n",
       "793                                  Blind-Side-The   \n",
       "794                                      Croods-The   \n",
       "795                                   Trainspotting   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [go, two, town, every, video, confess, make, a...   \n",
       "1    [sacred, teaching, white, bury, gave, grandfat...   \n",
       "2    [pretty, gas, way, cut, mountain, filter, engi...   \n",
       "3    [work, cabin, mind, friendship, find, main, ta...   \n",
       "4    [afraid, happen, make, right, born, thing, old...   \n",
       "..                                                 ...   \n",
       "791  [radio, give, argument, gun, would, last, ago,...   \n",
       "792  [bit, dance, make, test, holding, celebrate, c...   \n",
       "793  [investigate, trouble, bit, find, granger, fil...   \n",
       "794  [find, last, hope, would, every, three, fun, n...   \n",
       "795  [television, opener, family, compact, machine,...   \n",
       "\n",
       "                                                  text  \\\n",
       "0    go two town every video confess make account o...   \n",
       "1    sacred teaching white bury gave grandfather ta...   \n",
       "2    pretty gas way cut mountain filter engine gues...   \n",
       "3    work cabin mind friendship find main talk grad...   \n",
       "4    afraid happen make right born thing old give f...   \n",
       "..                                                 ...   \n",
       "791  radio give argument gun would last ago pam thr...   \n",
       "792  bit dance make test holding celebrate child cr...   \n",
       "793  investigate trouble bit find granger file wind...   \n",
       "794  find last hope would every three fun neighbor ...   \n",
       "795  television opener family compact machine elect...   \n",
       "\n",
       "                                                scenes  \n",
       "0    [[go, two, town, neighboring, every, video, co...  \n",
       "1    [[sacred, teaching, white, bury, gave, pah, gr...  \n",
       "2    [[pretty], [gas, way, get, cut, mountain, filt...  \n",
       "3    [[work, cabin, mind, friendship, find, know, m...  \n",
       "4    [[afraid, know, happen, make, right, born, thi...  \n",
       "..                                                 ...  \n",
       "791  [[radio, give, argument, gun], [would, last, a...  \n",
       "792  [[bit, dance, make, test, framing, holding, ce...  \n",
       "793  [[investigate], [trouble, bit, find, know, gra...  \n",
       "794  [[find, last, hope, would, every, three, fun, ...  \n",
       "795  [[television, opener, family, compact, machine...  \n",
       "\n",
       "[796 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token filtering and cleaning for movie-level analysis\n",
    "\n",
    "text_vals = df.text.values\n",
    "countVect = CountVectorizer()\n",
    "countVect = countVect.fit(text_vals)\n",
    "X_cv = countVect.transform(text_vals)\n",
    "pdCV = pd.DataFrame(X_cv.toarray(), columns=countVect.get_feature_names_out())\n",
    "pdCV[pdCV > 0] = 1\n",
    "pdCV = pdCV.loc[:,(pdCV.sum(axis=0) > 15)]\n",
    "pdCV = pdCV.loc[:,(pdCV.sum(axis=0) < pdCV.shape[0]/2)]\n",
    "good_tokens = set(pdCV.columns.values)\n",
    "\n",
    "def cleanerTokens(tokens):\n",
    "    return [token for token in tokens if token in good_tokens]\n",
    "\n",
    "df['scrubbed_tokens'] = df.apply(lambda x: cleanerTokens(x.tokens), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_filename</th>\n",
       "      <th>tokens</th>\n",
       "      <th>text</th>\n",
       "      <th>scenes</th>\n",
       "      <th>scrubbed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Easy-A</td>\n",
       "      <td>[go, two, town, every, video, confess, make, a...</td>\n",
       "      <td>go two town every video confess make account o...</td>\n",
       "      <td>[[go, two, town, neighboring, every, video, co...</td>\n",
       "      <td>[video, confess, account, occasional, record, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Killers-Of-The-Flower-Moon-Read-The-Screenplay</td>\n",
       "      <td>[sacred, teaching, white, bury, gave, grandfat...</td>\n",
       "      <td>sacred teaching white bury gave grandfather ta...</td>\n",
       "      <td>[[sacred, teaching, white, bury, gave, pah, gr...</td>\n",
       "      <td>[sacred, teaching, bury, grandfather, taught, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cast-Away</td>\n",
       "      <td>[pretty, gas, way, cut, mountain, filter, engi...</td>\n",
       "      <td>pretty gas way cut mountain filter engine gues...</td>\n",
       "      <td>[[pretty], [gas, way, get, cut, mountain, filt...</td>\n",
       "      <td>[gas, mountain, filter, engine, dirty, fuel, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ghost-Ship</td>\n",
       "      <td>[work, cabin, mind, friendship, find, main, ta...</td>\n",
       "      <td>work cabin mind friendship find main talk grad...</td>\n",
       "      <td>[[work, cabin, mind, friendship, find, know, m...</td>\n",
       "      <td>[cabin, friendship, main, graduate, voyage, qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Downsizing</td>\n",
       "      <td>[afraid, happen, make, right, born, thing, old...</td>\n",
       "      <td>afraid happen make right born thing old give f...</td>\n",
       "      <td>[[afraid, know, happen, make, right, born, thi...</td>\n",
       "      <td>[born, colleague, agree, impressive, pleasure,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>Bourne-Ultimatum-The</td>\n",
       "      <td>[radio, give, argument, gun, would, last, ago,...</td>\n",
       "      <td>radio give argument gun would last ago pam thr...</td>\n",
       "      <td>[[radio, give, argument, gun], [would, last, a...</td>\n",
       "      <td>[radio, argument, pam, confirmed, tape, locati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>Happy-Go-Lucky</td>\n",
       "      <td>[bit, dance, make, test, holding, celebrate, c...</td>\n",
       "      <td>bit dance make test holding celebrate child cr...</td>\n",
       "      <td>[[bit, dance, make, test, framing, holding, ce...</td>\n",
       "      <td>[dance, test, celebrate, cross, text, apart, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>Blind-Side-The</td>\n",
       "      <td>[investigate, trouble, bit, find, granger, fil...</td>\n",
       "      <td>investigate trouble bit find granger file wind...</td>\n",
       "      <td>[[investigate], [trouble, bit, find, know, gra...</td>\n",
       "      <td>[investigate, file, investigator, evidence, od...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>Croods-The</td>\n",
       "      <td>[find, last, hope, would, every, three, fun, n...</td>\n",
       "      <td>find last hope would every three fun neighbor ...</td>\n",
       "      <td>[[find, last, hope, would, every, three, fun, ...</td>\n",
       "      <td>[neighbor, fat, instant, forever, breakfast, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Trainspotting</td>\n",
       "      <td>[television, opener, family, compact, machine,...</td>\n",
       "      <td>television opener family compact machine elect...</td>\n",
       "      <td>[[television, opener, family, compact, machine...</td>\n",
       "      <td>[television, opener, compact, machine, electri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>796 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     movie_filename  \\\n",
       "0                                            Easy-A   \n",
       "1    Killers-Of-The-Flower-Moon-Read-The-Screenplay   \n",
       "2                                         Cast-Away   \n",
       "3                                        Ghost-Ship   \n",
       "4                                        Downsizing   \n",
       "..                                              ...   \n",
       "791                            Bourne-Ultimatum-The   \n",
       "792                                  Happy-Go-Lucky   \n",
       "793                                  Blind-Side-The   \n",
       "794                                      Croods-The   \n",
       "795                                   Trainspotting   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [go, two, town, every, video, confess, make, a...   \n",
       "1    [sacred, teaching, white, bury, gave, grandfat...   \n",
       "2    [pretty, gas, way, cut, mountain, filter, engi...   \n",
       "3    [work, cabin, mind, friendship, find, main, ta...   \n",
       "4    [afraid, happen, make, right, born, thing, old...   \n",
       "..                                                 ...   \n",
       "791  [radio, give, argument, gun, would, last, ago,...   \n",
       "792  [bit, dance, make, test, holding, celebrate, c...   \n",
       "793  [investigate, trouble, bit, find, granger, fil...   \n",
       "794  [find, last, hope, would, every, three, fun, n...   \n",
       "795  [television, opener, family, compact, machine,...   \n",
       "\n",
       "                                                  text  \\\n",
       "0    go two town every video confess make account o...   \n",
       "1    sacred teaching white bury gave grandfather ta...   \n",
       "2    pretty gas way cut mountain filter engine gues...   \n",
       "3    work cabin mind friendship find main talk grad...   \n",
       "4    afraid happen make right born thing old give f...   \n",
       "..                                                 ...   \n",
       "791  radio give argument gun would last ago pam thr...   \n",
       "792  bit dance make test holding celebrate child cr...   \n",
       "793  investigate trouble bit find granger file wind...   \n",
       "794  find last hope would every three fun neighbor ...   \n",
       "795  television opener family compact machine elect...   \n",
       "\n",
       "                                                scenes  \\\n",
       "0    [[go, two, town, neighboring, every, video, co...   \n",
       "1    [[sacred, teaching, white, bury, gave, pah, gr...   \n",
       "2    [[pretty], [gas, way, get, cut, mountain, filt...   \n",
       "3    [[work, cabin, mind, friendship, find, know, m...   \n",
       "4    [[afraid, know, happen, make, right, born, thi...   \n",
       "..                                                 ...   \n",
       "791  [[radio, give, argument, gun], [would, last, a...   \n",
       "792  [[bit, dance, make, test, framing, holding, ce...   \n",
       "793  [[investigate], [trouble, bit, find, know, gra...   \n",
       "794  [[find, last, hope, would, every, three, fun, ...   \n",
       "795  [[television, opener, family, compact, machine...   \n",
       "\n",
       "                                       scrubbed_tokens  \n",
       "0    [video, confess, account, occasional, record, ...  \n",
       "1    [sacred, teaching, bury, grandfather, taught, ...  \n",
       "2    [gas, mountain, filter, engine, dirty, fuel, s...  \n",
       "3    [cabin, friendship, main, graduate, voyage, qu...  \n",
       "4    [born, colleague, agree, impressive, pleasure,...  \n",
       "..                                                 ...  \n",
       "791  [radio, argument, pam, confirmed, tape, locati...  \n",
       "792  [dance, test, celebrate, cross, text, apart, h...  \n",
       "793  [investigate, file, investigator, evidence, od...  \n",
       "794  [neighbor, fat, instant, forever, breakfast, b...  \n",
       "795  [television, opener, compact, machine, electri...  \n",
       "\n",
       "[796 rows x 5 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
